# AIGC

text-3d

## DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION

使用diffusion的先验来实现nerf的调优从而生成三维的图像

## Magic3D（英伟达版dreamfusion，但是做的更好）

**DreamBooth**: 基于stable diffusion的finetune 方法，只需要少量图片和对应的标签，就能够finetune出该对象相关的图片。

## Generative Novel View Synthesis with 3D-Aware Diffusion Models （2023April NVIDIA&Stanford）

基于diffusion 模型的三维视角生成。主要创新在基于现有的2d 扩散框架下加入了三维的feature volume。latent feature可以有效提高生成一致性图像的能力。

related work的分析：有两种方法：一种是基于geometry的方法，比如nerf之类的。他们只能做好相邻视角的，对于更远的话就很模糊了。另一种方法是基于生成式的方法，他们能够生成比较好的细节，但是生成的不一定和输入的图片一致。

方法上其实比较接近pixelnerf，也是对每个volume都设计一个feature学习，只是把这个feature和noise图片concate一起后传入一个diffusion model，然后得到最终的效果。

重点是有了一个显式的三维表示然后加上了diffusion的方法。

## Adding Conditional Control to Text-to-Image Diffusion Models （ControlNet）

在解决text to image问题的时候常常有三个问题：是否基于promt的能够满足需求了？大模型能够适应一些定制化的任务吗？如果设置网络来应对不同格式的问题设置和使用者的控制？基于上述问题有三个发现：一是客制化任务的数据集常常是比较小的。而是data-driven的解决方法中大量的算力或许不是必要的。可以通过fintue或者迁移学习的方法来解决。三是问题的定义，控制的方式是多种多样的。要在许多任务中实现学习解决方案，端到端学习是必不可少的。

解决方式：提出了ControlNet，使用了大规模diffusion model，复制两份，一份frozen一份用来可训练。这两部分用一个独特的卷积层“zero convolution”来连接。convolution的权重以可学习的方式从0开始逐渐增加，从而优化了网络参数。由于zero convolution没有增加新的噪声到feature中，所以训练比较快，相当于finetune。

## Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models（CVPR2023）

## SKED: Sketch-guided Text-based 3D Editing

## NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors (CVPR2023)

## ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation (2023.5)

Score distillation sampling是text-to-3d中用来提炼预训练的diffusion model，但是这些SDS导致 over saturation， over smooth 和low diversity problem。因此提出variational score distillationn（VSD）

## DiffRoom: Diffusion-based High-Quality 3D Room Reconstruction and Generation with Occupancy Prior （2023.6 ）

之前的diffusion还没做过重建。因此这个论文提出了给定一个稀疏的occupancy prior来基于diffusion model重建的方法。这个方法是基于TSDF（Truncated Signed Distance Field）的。
